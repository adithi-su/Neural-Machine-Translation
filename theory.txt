Machine translation systems
--------------------------------------------------------------------------------------------------------
# Rule-based machine translation-
Rules working at syntactic, semantic and lexical levels. Three phases-
1. Analysis Phase
2. Lexical Phase
3. Generation Phase

Flow Diagram- 
English:It's a beautiful day -> Morphological analyzer -> Parts of Speech Tagger and Chunker
-> Named Entity Recognition -> Word Sense Disambiguation -> Lexical Transfer -> Word Generator
-> French:C'est une belle journe`e 

1. Analysis Phase
	-Source language text is analyzed to extract info about morphology, parts of speech,named entity recognition and word sense disambiguation
	- Morphology:  structure of words, how their stems are derived, detection of rootwords etc
	- Part of speech tagger: tags nouns,adjectives,adverbs etc.
	- NER: tries to classify named entities into predefined buckets (ex name of person,location, name of org, etc) 
	- Word-sense disambiguation: tries to identify how a particular word has been used in a sentence
2. Lexical transfer Phase
	- Word translation: using bilingual translation dict
	- Grammar translation: syntactic modification is performed, including translating the suffixes
3. Generation Phase
	- translated text is validated and corrected wrt parts of speech, gender, and agreement of subject and object, wrt verb before providing an output
	- makes use of predefined dictionaries:
		- dict for source lang morphological analysis
		- bilingual dict containing mappings of the source word to target language counterpart
		- dict containing target-lang morphological info for target word detection
-----------------------------------------------------------------------------------------------------------------------------------------------------------
# Statistical ML systems
Select target by maximizing conditional probability, given the source text
t^ = argmax P(t/s)
t^ = argmax ( P(s/t)P(t) )/P(s)
t^ = argmax P(s/t)P(t) as for a given source sentence P(s) would be fixed
		- argmax component: Search algorithm 
		- P(s/t) component: Translation model
		- P(t): Language model for Target
ill formed sentences that are highly likely under P(t/s) are avoided by breaking the problem into 2 comps - P(s/t)and P(t)

1.Language model: Probability of sentence is represented as a product of the conditional probabilities of invidividual words/phrases

Say sentence t consists of words t1, t2 ... then, P(t) = P(t1t2....tn) = P(t1)P(t2/t1)P(t3/t1t2).....P(tn/t1..tn-1)
Not exactly to feasible to calculate each of those probabilities -> use Markov Assumption 
Markov assumption, bigram model - assumption is to condition a word based on the previous word, not all the preceding words
Conditional prob of bigram model- P(tn/t1t2...tn-1) = P(tn/tn-1) 

The cond prob of next word being t2, given current word is t1 can be estimated by counting the total number of pairs of (t1,t2) in training and normalizing 
the total occurences of the word t1: P(t2/t1) = P(t1,t2)/P(t1) = count(t1,t2)/count(t1)

further improvement-trigram model: P(tn/t1t2...tn-1) = P(tn/tn-1tn-2) 
P(t3/t1t2) = count(t1,t2,t3)/count(t1,t2)

beyond trigram model -> leads to sparcity

$Perplexity metrics->to evaluate usefullness of lang model
Perplexity of a model P(.) is evaluated on a test set(w1,w2,....wm) drawn from the same population as that of the training set-
PP = 2^H = 2 ^{-(1/M)logbase2P(w1,w2,....wm)

2.Translational Model:estimate P(s/t)
		2.1 Fertility:prob distribution over the number of words generated by a source-lang word in the target lang, rep as P(n/Ws) where Ws-source word
			Instead of hardcoding n, prob dist is used to as same word might generate translations of different length based on context
		2.2 Distortion:covers the notion of alignment 
			P(pt/ps,l) where pt and ps -position of target and source, l-length of target sentence
		2.3 Word-to-Word translation: P(Wt/Ws)
------------------------------------------------------------------------------------------------------------------------------------------------------------------
# Neural Machine Translation

NMT machine takes text in source lang as a sequence of inputs, encodes these to a hidden representation, which is then decoded back to produce translated text in
target language
Adv: whole machine translation system can be trained from end-to-end together
Mostly used: RNN architecture (LSTMs and GRUs)

								     [START]C'est une belle journe`e 
It's a beautiful day -> LSTM Encoder ->Internal LSTM states (h,c) -> LSTM Decoder
								     C'est une belle journe`e [STOP]



